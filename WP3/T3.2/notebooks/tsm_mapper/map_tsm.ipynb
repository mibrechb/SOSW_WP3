{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba1e38-b579-4750-8dfa-b9503c475539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d03778-c932-4320-83e5-a97cb7e7347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e7929-604e-4300-9c87-8cc78ead5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map(center=(40, -100), zoom=4)\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f80ff4-05a6-4725-b7ee-71e5e891b6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "def get_mrc_metadata(return_gdf=True, verbose=False):\n",
    "    \"\"\" Get Metadata from Mekong River Comission Data Portal. \"\"\"\n",
    "    url = r'https://api.mrcmekong.org/api/v1/ts/inventory/timeSeriesList'\n",
    "    urllib.request.urlretrieve(url, 'timeSeriesList.json')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Downloaded time-series metadata from {url} .')\n",
    "    \n",
    "    f = open('timeSeriesList.json')\n",
    "    data = json.load(f)\n",
    "    df_metadata = pd.DataFrame([])\n",
    "    for dataset in data:\n",
    "        df_temp = pd.DataFrame([dict(dataset)])\n",
    "        df_temp['longitude'] = df_temp['longitude'].astype(float)\n",
    "        df_temp['latitude'] = df_temp['latitude'].astype(float)\n",
    "        df_metadata = pd.concat([df_temp, df_metadata])\n",
    "        f.close()\n",
    "    df_metadata = df_metadata.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Found a total of {df_metadata.shape[0]} time-series datasets from {len(df_metadata.stationCode.unique())} stations of the MRC Data Portal.')\n",
    "        \n",
    "    if return_gdf:\n",
    "        gdf_metadata = gpd.GeoDataFrame(\n",
    "            df_metadata, geometry=gpd.points_from_xy(df_metadata.longitude, df_metadata.latitude), crs=\"EPSG:4326\"\n",
    "        )\n",
    "        return(gdf_metadata)\n",
    "    else:\n",
    "        return(df_metadata)\n",
    "\n",
    "gdf_metadata = get_mrc_metadata(return_gdf=True)\n",
    "gdf_metadata_dmsp = gdf_metadata.loc[gdf_metadata.label.str.contains('DSMP')]\n",
    "gdf_stations_dsmp = gdf_metadata_dmsp.groupby('locationIdentifier').first()[['river', 'stationShortName', 'geometry']].set_crs('EPSG:4326')\n",
    "Map.add_gdf(gdf_stations_dsmp, 'MRC DSMP stations', {'color': 'red'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d34ab3-e735-4033-99a0-02fee254af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from DSMP surves\n",
    "paths_data_s = list(Path(f'../mrc_webscrapper/outputs/csv/Sediment Concentration/').glob(f'*.csv'))\n",
    "paths_data_q = list(Path(f'../mrc_webscrapper/outputs/csv/Discharge/').glob(f'*.csv'))\n",
    "paths_data = paths_data_q + paths_data_s\n",
    "df_data = pd.DataFrame([])\n",
    "for path in paths_data_s:\n",
    "    df_temp = pd.read_csv(path, dtype={'station_code':'str'})\n",
    "    df_temp['date_utc'] = pd.to_datetime(df_temp['date'])\n",
    "    df_temp['med_frq'] = np.median(np.diff(df_temp.date_utc))\n",
    "    df_data = pd.concat([df_data, df_temp])\n",
    "\n",
    "df_data_dsmp = df_data.loc[df_data.identifier.str.contains('DSMP')]\n",
    "df_dsmp_stations = df_data_dsmp.groupby('station_code').first()\n",
    "df_data_dsmp = df_data.loc[df_data.station_code.isin(df_dsmp_stations.index)]\n",
    "    \n",
    "gdf_stations = gpd.GeoDataFrame(df_dsmp_stations,\n",
    "                 crs={'init': 'epsg:4326'},\n",
    "                 geometry=df_dsmp_stations.apply(lambda row: shapely.geometry.Point((row.lon, row.lat)), axis=1)\n",
    "                )\n",
    "\n",
    "gdf_data_dsmp = gpd.GeoDataFrame(df_data_dsmp.join(gdf_stations.geometry, on='station_code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70045cec-b6d3-4267-b552-2dafb83a07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timespan\n",
    "start_date, end_date = '2000-01-01', '2024-12-31'\n",
    "\n",
    "# Cloud masking (scene-based)\n",
    "cld_filt_thresh = 80        # Maximum image cloud cover percent allowed in image collection\n",
    "\n",
    "# water masking\n",
    "mask_water = True\n",
    "\n",
    "# # Cloud masking (pixel-based, s2cloudless only)\n",
    "# cld_prb_thresh = 25      # Cloud probability (%); values greater than are considered cloud\n",
    "# cld_prj_dist = 1          # Maximum distance (km) to search for cloud shadows from cloud edges (based on Hollstein decision tree)\n",
    "\n",
    "# Cloud masking (pixel-based, cloud score+ only)\n",
    "qa_band = 'cs_cdf'\n",
    "clear_thresh = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c72da-3ffe-4b9f-aa9a-3b65dce4a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 3S basin\n",
    "json_data = 'geometries/geoms.geojson'\n",
    "fc_geoms = geemap.geojson_to_ee(json_data)\n",
    "roi_geom = fc_geoms.first().geometry()\n",
    "\n",
    "# load dams\n",
    "df = pd.read_csv('geometries/3SReservoirs.csv')\n",
    "gdf_dams = gpd.GeoDataFrame(df, geometry=gpd.GeoSeries.from_xy(df['X'], df['Y']), crs=4326).drop(columns=['X', 'Y']).set_index('id')\n",
    "fc_dams = geemap.geopandas_to_ee(gdf_dams)\n",
    "\n",
    "# define stations\n",
    "stations = ee.FeatureCollection([\n",
    "  ee.Feature(ee.Algorithms.GeometryConstructors.Point([106.39220569493087, 14.11961739545873]), {'station_id': '430102', 'station_name': 'Siempang'}),\n",
    "  ee.Feature(ee.Algorithms.GeometryConstructors.Point([107.78237137912318, 12.897991717109619]), {'station_id': '451305', 'station_name': 'Ban Don'}),\n",
    "  ee.Feature(ee.Algorithms.GeometryConstructors.Point([106.5278998223061, 13.553039642985812]), {'station_id': '450101', 'station_name': 'Lum Phat'}),\n",
    "  ee.Feature(ee.Algorithms.GeometryConstructors.Point([107.47041987706022, 13.940093424442852]), {'station_id': '440202', 'station_name': 'Pleiku'}),\n",
    "  ee.Feature(ee.Algorithms.GeometryConstructors.Point([107.10639453530615, 14.050489408398247]), {'station_id': '440103', 'station_name': 'Andaung Meas'}),\n",
    "  ee.Feature(ee.Algorithms.GeometryConstructors.Point([107.44760623609672, 13.792268298200513]), {'station_id': '440100', 'station_name': 'Phum Pi'}),\n",
    "  ee.Feature(ee.Algorithms.GeometryConstructors.Point([105.987503299606, 13.537303972542501]), {'station_id': '014501', 'station_name': 'Stung Treng'}),\n",
    "])\n",
    "\n",
    "lake_rhone = ee.Feature(ee.FeatureCollection('users/michaelbrechbuehler/Landsat_ST/shapefiles/rhone_wgs84').first().geometry().centroid(), {'station_id': 'Rhonesee'})\n",
    "lake_steisee = ee.Feature(ee.FeatureCollection('users/michaelbrechbuehler/Landsat_ST/shapefiles/steisee_wgs84').first().geometry().centroid(), {'station_id': 'Steinsee'})\n",
    "\n",
    "stations = lake_rhone\n",
    "\n",
    "# # add S2 clear median\n",
    "# filters = ee.Filter.And(\n",
    "#     ee.Filter.bounds(roi_geom), \n",
    "#     ee.Filter.date('2017-12-01', '2019-03-31'),\n",
    "#     ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)\n",
    "# )\n",
    "# s2_median = ee.ImageCollection(\"COPERNICUS/S2_SR\").filter(filters).median()\n",
    "\n",
    "# Map.addLayer(s2_median, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 2000}, 'Sentinel-2 RGB')\n",
    "#Map.addLayer(roi_geom, {'color': 'yellow'}, '3S Basin')\n",
    "Map.addLayer(stations, {'color': 'red'}, 'Stations')\n",
    "Map.add_gdf(gdf_dams, 'Dams', {'color': 'blue'})\n",
    "Map.centerObject(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24610c-8a86-4111-beac-b1ce01853ddb",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc80a5-7307-440f-96da-1f65f98a327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_station(ic_rs, max_diff=1):\n",
    "    def wrap(fc_station):\n",
    "        fc_matchups = get_matchups(fc_station, ic_rs, max_diff).map(get_sample)\n",
    "        return fc_matchups\n",
    "    return wrap\n",
    "\n",
    "def run_station_task(ic_rs, sensor, max_diff=1):\n",
    "    def wrap(fc_station):\n",
    "        fc_matchups = get_matchups(fc_station, ic_rs, max_diff).map(get_sample)\n",
    "        task = ee.batch.Export.table.toDrive(**{\n",
    "            'collection': fc_matchups, \n",
    "            'description': f'TSS_export_{sensor}',\n",
    "            'folder': rf'Earth Engine/TSM/{sensor}'})\n",
    "        task.start()\n",
    "        return task\n",
    "    return wrap\n",
    "\n",
    "def get_matchups(fc_station, ic_rs, max_diff=1):\n",
    "    \"\"\" Matches FeatureCollection with closest match from ImageCollection. \"\"\"\n",
    "    geometry = ee.FeatureCollection(fc_station).geometry()\n",
    "    ic_rs = ic_rs.filter(ee.Filter.bounds(geometry))\n",
    "    max_diff_filter = ee.Filter.maxDifference(**{\n",
    "      'difference': max_diff * 24 * 60 * 60 * 1000,\n",
    "      'leftField': 'system:time_start',\n",
    "      'rightField': 'system:time_start'\n",
    "    })\n",
    "    save_best_join = ee.Join.saveBest(**{\n",
    "      'matchKey': 'bestImage',\n",
    "      'measureKey': 'timeDiff'\n",
    "    })\n",
    "    fc_matchups = save_best_join.apply(fc_station, ic_rs, max_diff_filter);\n",
    "    return fc_matchups\n",
    "\n",
    "def get_sample(feature):\n",
    "    \"\"\" Sample matched image at feature geometry and add aggregated value as property. \"\"\"\n",
    "    feature = ee.Feature(feature)\n",
    "    match_img = ee.Image(feature.get('bestImage'))\n",
    "    geometry = feature.geometry().buffer(200)\n",
    "    value = feature.get('value')\n",
    "    samples_agg = match_img.reduceRegion(reducer=ee.Reducer.median(), geometry=geometry)\n",
    "    feature = feature.set('values_eo', samples_agg).set('bestImage', match_img.get('system:index')) \n",
    "    return(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d176b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a30c4-3b9c-4701-91e6-9aac884f5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import functions_process as funcs_process\n",
    "import functions_turbidity as funcs_turb\n",
    "import functions_sampling as funcs_sampling\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# set timespan\n",
    "start_date, end_date = '2000-01-01', '2024-12-31'\n",
    "\n",
    "# prepare insitu data\n",
    "df = gpd.read_file('input/insitu_data.csv', ignore_geometry=True)\n",
    "df['geometry'] = gpd.GeoSeries.from_wkt(df['geometry']).set_crs('4326')\n",
    "gdf_data = gpd.GeoDataFrame(df)\n",
    "gdf_data = gdf_data.loc[~gdf_data.geometry.is_empty]\n",
    "#gdf_data = gdf_data.loc[gdf_data.source=='WQMN']\n",
    "\n",
    "tasks = []\n",
    "for identifier in tqdm(gdf_data.identifier.unique()):\n",
    "    fn = identifier.split('@')[1].replace(' ', '').replace('[', '').replace(']', '').replace('(', '').replace(')', '')\n",
    "    gdf_data_station = gdf_data.loc[gdf_data.identifier==identifier]\n",
    "    gdf_data_station['date'] = gdf_data_station.date.apply(lambda x: str(x)[:19])\n",
    "    fc_station = ee.FeatureCollection(geemap.gdf_to_ee(\n",
    "        gdf_data_station, \n",
    "        date='date', date_format='YYYY-MM-dd HH:mm:ss'))\n",
    "    bounds = fc_station.geometry()\n",
    "    ic_oli = funcs_process.load_sr_imcoll(sensor='oli', start_date=start_date, end_date=end_date, bounds=bounds, watermask='qa')\n",
    "    ic_etm = funcs_process.load_sr_imcoll(sensor='etm', start_date=start_date, end_date=end_date, bounds=bounds, watermask='qa')\n",
    "    ic_msi = funcs_process.load_sr_imcoll(sensor='msi', start_date=start_date, end_date=end_date, bounds=bounds, watermask='scl')\n",
    "    fc_matchups_oli = get_matchups(fc_station, ic_oli, max_diff=1)\n",
    "    fc_matchups_msi = get_matchups(fc_station, ic_msi, max_diff=1)\n",
    "    fc_matchups_etm = get_matchups(fc_station, ic_etm, max_diff=1)\n",
    "    fc_matchups = ee.FeatureCollection([fc_matchups_oli, fc_matchups_msi, fc_matchups_etm]).flatten().map(get_sample)\n",
    "    task = ee.batch.Export.table.toDrive(**{\n",
    "        'collection': fc_matchups, \n",
    "        'description': f'TSS_{fn}',\n",
    "        'folder': 'Earth Engine/TSS'})\n",
    "    task.start()\n",
    "    tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd2741-8d92-4a60-8e3d-8638c0167931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_tasks_status(tasks):\n",
    "    \"\"\" Check the state of all provided ee.task objects and posts status updates. \"\"\"\n",
    "    colordict = {'white': '\\033[0m', 'red': '\\033[91m', 'orange': '\\033[93m', 'green': '\\033[92m'}\n",
    "    states = []\n",
    "    for task in tasks:\n",
    "        # get state and times\n",
    "        status = task.status()\n",
    "        state = status['state']\n",
    "        task_id = status['id']\n",
    "        time_start, time_update = status['creation_timestamp_ms'], status['update_timestamp_ms']\n",
    "        time_elapsed = timedelta(milliseconds=(time_update-time_start))\n",
    "        time_now = datetime.now()\n",
    "        # set output color\n",
    "        if state == 'COMPLETED':\n",
    "            color ='green'\n",
    "        elif (state == 'RUNNING') | (state == 'READY'):\n",
    "            color = 'orange'\n",
    "        elif (state == 'FAILED') | (state == 'CANCEL_REQUESTED') | (state == 'CANCELLED'):\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'white'\n",
    "        # print msg\n",
    "        status_msg = f\"[{str(time_now)[:19]}] Task {task_id}\" \\\n",
    "                     f\"({status.get('description', 'No description')}): {colordict[color]+state+colordict['white']}\"\n",
    "                     #f\" (runtime: {time_elapsed.seconds/60:0.1f}min)\"\n",
    "        print(status_msg)\n",
    "        states.append(state)\n",
    "    return states\n",
    "\n",
    "# tasks = []\n",
    "# for sensor, fc in [('oli', fc_matchups_oli), ('etm', fc_matchups_etm), ('msi', fc_matchups_msi)]:\n",
    "#     task = ee.batch.Export.table.toDrive(**{\n",
    "#         'collection': fc, \n",
    "#         'description': f'TSS_export_{sensor}',\n",
    "#         'folder': 'Earth Engine'})\n",
    "#     task.start()\n",
    "#     tasks.append(task)\n",
    "\n",
    "all_completed = False\n",
    "while not all_completed:\n",
    "    check_tasks_status(tasks)\n",
    "    states = [task.status()['state'] for task in tasks]\n",
    "    if all(state in ['COMPLETED', 'FAILED', 'CANCEL_REQUESTED'] for state in states):\n",
    "        all_completed = True\n",
    "    else:\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"All export tasks finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def monitor_ee_tasks(tasks, check_interval=30):\n",
    "    \"\"\"\n",
    "    Monitor a list of GEE tasks and display tqdm.notebook progress bars for each.\n",
    "\n",
    "    Parameters:\n",
    "    - tasks: A list of ee.task objects to monitor.\n",
    "    - check_interval: Time in seconds between status checks.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to hold our progress bars\n",
    "    progress_bars = {}\n",
    "\n",
    "    # Initialize progress bars for each task\n",
    "    for task in tasks:\n",
    "        status = task.status()\n",
    "        progress_bars[status['id']] = tqdm(total=100, desc=f\"Task {status['description']}\")\n",
    "    try:\n",
    "        tasks_complete = []\n",
    "        while len(tasks)!=len(tasks_complete):\n",
    "            statuses = [task.status() for task in tasks]            \n",
    "            for status in statuses:\n",
    "                id = status['id']\n",
    "                if status['state'] in ['FAILED', 'CANCELLED', 'CANCEL_REQUESTED']:\n",
    "                        progress_bars[id].bar_style = 'danger'\n",
    "                elif status['state'] in ['COMPLETED']:\n",
    "                    progress_bars[id].n = 100\n",
    "                    progress_bars[id].refresh()\n",
    "                    progress_bars[id].close()\n",
    "                    tasks_complete.append(status['id'])\n",
    "                else:\n",
    "                    progress = status.get('progress', 0)\n",
    "                    progress_bars[id].n = progress\n",
    "                    progress_bars[id].refresh()            \n",
    "            # Wait for a bit before checking the status again\n",
    "            time.sleep(check_interval)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Monitoring interrupted.\")\n",
    "\n",
    "monitor_ee_tasks(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723c838-a2c4-4eab-9711-5657a7318835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comput tsm features\n",
    "# ic_all = ic_all \\\n",
    "#     .map(funcs_turb.calc_spm_nechad) \\\n",
    "#     .map(funcs_turb.calc_tur_nechad) \\\n",
    "#     .map(funcs_turb.calc_tur_dogliotti) \\\n",
    "#     .map(funcs_turb.calc_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cdebb6-9e61-494d-a842-bbb30c9569c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = ic_msi.first().select(0).projection().crs()\n",
    "scale = 30\n",
    "\n",
    "# wxee convert to xarray\n",
    "#ds_msi = ic_msi.select('B4').limit(25).wx.to_xarray(scale=scale, region=bounds)\n",
    "\n",
    "# geemap export\n",
    "geemap.ee_export_image_collection_to_drive(ee.ImageCollection(ic_msi).select('B4'), folder='export/oli', maxPixels=200000000, region=bounds, scale=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9b0c9-e49c-41c3-b399-43aa932d0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export red bands to geotiffs\n",
    "geemap.ee_export_image_collection_to_drive(ee.ImageCollection(imcoll_etm).select('B3'), folder='export/msi', maxPixels=200000000, region=bounds, scale=30)\n",
    "geemap.ee_export_image_collection_to_drive(ee.ImageCollection(imcoll_oli).select('B4'), folder='export/oli', maxPixels=200000000, region=bounds, scale=30)\n",
    "geemap.ee_export_image_collection_to_drive(ee.ImageCollection(imcoll_msi).select('B4'), folder='export/oli', maxPixels=200000000, region=bounds, scale=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
