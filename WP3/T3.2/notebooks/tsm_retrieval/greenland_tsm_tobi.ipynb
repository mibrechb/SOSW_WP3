{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34011ec1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a765dd",
   "metadata": {},
   "source": [
    "## Load TSM RS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(Path('data/greenland_tobi').glob(f'rrs_main_timeseries_tobi.csv'))\n",
    "path_csv = paths[0]\n",
    "df_rs = pd.read_csv(path_csv, dtype={'station_id':str})\n",
    "df_rs['dt_utc'] = pd.to_datetime(df_rs['system:time_start'], format='mixed', utc=True)\n",
    "df_rs['date'] = df_rs['dt_utc'].dt.date\n",
    "df_rs = df_rs.drop(columns=['.geo', 'system:time_start'], errors='ignore')\n",
    "df_rs = df_rs.sort_values('date')\n",
    "print(f'Loaded \"{path_csv}\" file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rs.loc[(df_rs.station_id=='kommuneqarfik') & (df_rs.platform=='LANDSAT-8')].hvplot.scatter(x='date', y='add_ndssi_median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f78b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = list(Path('validation').glob(f'sr_timeseries_mrc.csv'))\n",
    "# path_csv = paths[0]\n",
    "# df_rs = pd.read_csv(path_csv, dtype={'station_id':str})\n",
    "# df_rs['dt_utc'] = pd.to_datetime(df_rs['system:time_start'], format='mixed', utc=True)\n",
    "# df_rs['dt_loc'] = df_rs['dt_utc'].dt.tz_convert('Asia/Ho_Chi_Minh')\n",
    "# df_rs['date'] = df_rs['dt_loc'].dt.date\n",
    "# df_rs = df_rs.drop(columns=['.geo', 'system:time_start'], errors='ignore')\n",
    "# df_rs = df_rs.sort_values('date')\n",
    "# df_rs['data_source'] = 'mrc'\n",
    "# df_rs_mrc = df_rs\n",
    "# print(f'Loaded \"{path_csv}\" file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rs = pd.concat([df_rs_siwrp, df_rs_mrc])\n",
    "df_rs = df_rs_siwrp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ebdbc",
   "metadata": {},
   "source": [
    "# Plot regression for all datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 'ALL'    # Set sensor (OLI/MSI/ALL)\n",
    "metric = 'add_ratio_gr_median' # Set metric\n",
    "\n",
    "# Set thresholds\n",
    "thresh_cloud_cover = 80\n",
    "thresh_roi_coverage = 85\n",
    "\n",
    "if sensor == 'OLI':\n",
    "    platforms = ['LANDSAT-8', 'LANDSAT-9']\n",
    "elif sensor == 'MSI':\n",
    "    platforms = ['SENTINEL-2']\n",
    "elif sensor == 'ALL':\n",
    "    platforms = ['LANDSAT-8', 'LANDSAT-9', 'SENTINEL-2']\n",
    "    \n",
    "print(f'Platforms: {platforms}')\n",
    "\n",
    "cmap = matplotlib.colors.ListedColormap(plt.cm.tab10.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style()\n",
    "\n",
    "# Filter matchups\n",
    "data = df_rs.loc[~np.isnan(df_rs.value)]\n",
    "data = data.loc[data.platform.isin(platforms)]\n",
    "data = data.loc[data.CLOUD_COVER<=thresh_cloud_cover]\n",
    "#data = data.loc[data.cloudiness_250m_mean<=0.5]\n",
    "data = data.loc[data.roi_coverage>=thresh_roi_coverage]\n",
    "\n",
    "data = data.loc[data[metric]>-0.2]\n",
    "\n",
    "\n",
    "g = sns.lmplot(x=metric, y='value', data=data, \n",
    "               #col='station_id', col_wrap=2,\n",
    "               #hue='platform',\n",
    "               #logx=True,\n",
    "               #sharex=True, sharey=True,\n",
    "               facet_kws={'sharey': False, 'sharex': False})\n",
    "\n",
    "def annotate(data, **kws):\n",
    "    r, p = sp.stats.pearsonr(data['value'], data[metric])\n",
    "    ax = plt.gca()\n",
    "    ax.text(.6, .9, 'r={:.2f}, p={:.2g}'.format(r, p),\n",
    "            transform=ax.transAxes)\n",
    "    \n",
    "g.map_dataframe(annotate)\n",
    "\n",
    "plt.ylabel(r'$TSM_{insitu}$ (mg/L)')\n",
    "plt.xlabel(metric)\n",
    "#plt.ylabel(r'$TSM_{Nechad}$ (mg/L)')\n",
    "#plt.title(f'Metric: {metric}')\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def f_GI_tss(x, c0, c1, c2):\n",
    "    Rb, Rg, Rr, Rnir = x\n",
    "    w1 = Rr/(Rr+Rnir)\n",
    "    w2 = Rnir/(Rr+Rnir)\n",
    "    GI_tss = c0 * (Rg/Rb) + c1 * w1 * (Rr/Rg) + c2 * w2 * (Rnir/Rg)\n",
    "    return GI_tss\n",
    "\n",
    "# get data\n",
    "Rb = data.B2_median.values\n",
    "Rg = data.B3_median.values\n",
    "Rr = data.B4_median.values\n",
    "Rnir = data.apply(lambda x: x['B5_median'] if (str(x['platform']).split('-')[0] == 'LANDSAT') else x['B8_median'], axis=1)\n",
    "\n",
    "x = (Rb, Rg, Rr, Rnir)\n",
    "y = data.value/1000\n",
    "\n",
    "p0 = (-0.028, 0.28, 0.95)\n",
    "bounds_h = (2, 2, 2)\n",
    "bounds_l = (-2, -2, -2)\n",
    "\n",
    "# perform the fit\n",
    "params, cv = scipy.optimize.curve_fit(f_GI_tss, x, y, p0, bounds=(bounds_l, bounds_h), maxfev=10000000)\n",
    "c0, c1, c2 = params\n",
    "print(c0, c1, c2)\n",
    "\n",
    "# determine quality of the fit\n",
    "squaredDiffs = np.square(y - f_GI_tss(x, c0, c1, c2))\n",
    "squaredDiffsFromMean = np.square(y - np.mean(y))\n",
    "rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "print(f\"R² = {rSquared}\")\n",
    "\n",
    "# Calculae GITTS\n",
    "for idx, row in data.iterrows():\n",
    "    platform = row.platform\n",
    "    Rb = row.B2_median\n",
    "    Rg = row.B3_median\n",
    "    Rr = row.B4_median\n",
    "    Rnir = row.B5_median if (platform.split('-')[0] == 'LANDSAT') else row.B8_median\n",
    "    x = (Rb, Rg, Rr, Rnir)\n",
    "    data.loc[idx, 'GI_tss'] = f_GI_tss(x, c0, c1, c2)\n",
    "    df_rs.loc[idx, 'GI_tss'] = f_GI_tss(x, c0, c1, c2)\n",
    "    \n",
    "# plot the results\n",
    "colors = list(data.platform.apply(lambda x: cmap(0) if x=='LANDSAT-8' else cmap(1)).values)\n",
    "plt.grid(which='major', axis='y', zorder=-1.0)\n",
    "plt.scatter(data.GI_tss.values, y, color=colors, label=\"data\")\n",
    "#plt.plot(data.sort_values('GI_tss').GI_tss, data.sort_values('GI_tss').GI_tss, 'k--', label=\"data\")\n",
    "plt.title(\"Fitted GI Curve\")\n",
    "plt.ylabel(r'$TSM_{insitu}$ (g/L)')\n",
    "plt.xlabel(r'$GI_{TSS}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "def twoTermExp(x, a, b, c, d):\n",
    "    return a*np.exp(b*x) + c*np.exp(d*x)\n",
    "\n",
    "metric = 'add_ratio_rgb_median'\n",
    "data = data.sort_values(metric)\n",
    "\n",
    "# get data\n",
    "xs = data[metric]\n",
    "ys = data.value\n",
    "\n",
    "# perform the fit\n",
    "p0 = (2000, .1, 50, 0) # start with values near those we expect\n",
    "params, cv = scipy.optimize.curve_fit(twoTermExp, xs, ys, p0, maxfev=100000)\n",
    "a, b, c, d = params\n",
    "\n",
    "# determine quality of the fit\n",
    "squaredDiffs = np.square(ys - twoTermExp(xs, a, b, c, d))\n",
    "squaredDiffsFromMean = np.square(ys - np.mean(ys))\n",
    "rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "print(f\"R² = {rSquared}, R = {np.sqrt(rSquared)}\")\n",
    "print(f'RMSE = {np.sqrt(metrics.mean_squared_error(ys, twoTermExp(xs, a, b, c, d)))}')\n",
    "\n",
    "# plot the results\n",
    "colors = list(data.platform.apply(lambda x: cmap(0) if x=='LANDSAT-8' else cmap(1)).values)\n",
    "plt.grid(which='major', axis='y', zorder=-1.0)\n",
    "plt.scatter(xs, ys, label=\"data\", color=colors)\n",
    "plt.plot(xs, twoTermExp(xs, a, b, c, d), 'k--', label=\"fitted\")\n",
    "plt.ylabel(r'$TSM_{insitu}$ (mg/L)')\n",
    "plt.xlabel(r'$(\\frac{1}{B_{red}}+\\frac{1}{B_{green}})*B_{blue}$')\n",
    "plt.title(\"Fitted Exponential Curve (Two-term)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def monoExp(x, m, t, b):\n",
    "    return m * np.exp(-t * x) + b\n",
    "\n",
    "metric = 'add_ratio_rgb_median'\n",
    "data = data.sort_values(metric)\n",
    "\n",
    "data = data.loc[data.value<400]\n",
    "\n",
    "# get data\n",
    "xs = data[metric]\n",
    "ys = data.value\n",
    "\n",
    "# perform the fit\n",
    "p0 = (7, 18, 22) # start with values near those we expect\n",
    "params, cv = scipy.optimize.curve_fit(monoExp, xs, ys, maxfev=100000)\n",
    "m, t, b = params\n",
    "\n",
    "# determine quality of the fit\n",
    "squaredDiffs = np.square(ys - monoExp(xs, m, t, b))\n",
    "squaredDiffsFromMean = np.square(ys - np.mean(ys))\n",
    "rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "print(f\"R² = {rSquared}\")\n",
    "print(f'RMSE = {np.sqrt(metrics.mean_squared_error(ys, monoExp(xs, m, t, b)))}')\n",
    "\n",
    "# plot the results\n",
    "colors = list(data.platform.apply(lambda x: cmap(0) if x=='LANDSAT-8' else cmap(1)).values)\n",
    "plt.grid(which='major', axis='y', zorder=-1.0)\n",
    "plt.scatter(xs, ys, label=\"data\", color=colors)\n",
    "plt.plot(xs, monoExp(xs, m, t, b), 'k--', label=\"fitted\")\n",
    "plt.ylabel(r'$TSM_{insitu}$ (mg/L)')\n",
    "plt.xlabel(r'$(\\frac{1}{B_{red}}+\\frac{1}{B_{green}})*B_{blue}$')\n",
    "\n",
    "plt.title(\"Fitted Exponential Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce950ed8",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "centerwl_lut_msi = {\n",
    "    'B1':442.7, 'B2':492.4, 'B3':559.8, 'B4':664.6, 'B5':704.1, 'B6':740.5,'B7':782.8, 'B8':832.8, 'B8A':864.7, 'B9':945.1, 'B10':1373.5, 'B11':1613.7, 'B12':2202.4\n",
    "}\n",
    "centerwl_lut_oli = {\n",
    "    'B1':442.96, 'B2':482.04, 'B3':561.41, 'B4':654.59, 'B5':864.67, 'B6':1608.86, 'B7':2200.73,'B8':590,'B9':1375,'B10':10800,'B11':1200  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb208df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "thresh_cloud_cover = 50\n",
    "thresh_roi_coverage = 90\n",
    "data = df_rs.copy()\n",
    "#data = data.drop(columns=)\n",
    "data = data.loc[data.CLOUD_COVER<=thresh_cloud_cover]\n",
    "data = data.loc[data.roi_coverage>=thresh_roi_coverage]\n",
    "data_msi = data.loc[data.platform=='SENTINEL-2']\n",
    "data_msi = data_msi[data_msi.columns[((data_msi.columns.str.startswith('B'))&(data_msi.columns.str.endswith('_median')))|(data_msi.columns=='value')]]\n",
    "data_oli = data.loc[data.platform=='LANDSAT-8']\n",
    "data_oli = data_oli[data_oli.columns[((data_oli.columns.str.startswith('B'))&(data_oli.columns.str.endswith('_median')))|(data_oli.columns=='value')]]\n",
    "\n",
    "# Compute a correlation matrix and convert to long-form\n",
    "corr_mat = data_msi.corr().stack().reset_index(name=\"correlation\")\n",
    "corr_mat = corr_mat.loc[corr_mat.level_0=='value'].reset_index().drop(columns=['level_0', 'index']).rename(columns={'level_1': 'band'})\n",
    "corr_mat_msi = corr_mat.loc[corr_mat.band!='value']\n",
    "corr_mat_msi['band'] = corr_mat_msi.band.apply(lambda x: x.split('_')[0])\n",
    "corr_mat_msi['wavelength'] = corr_mat_msi.band.apply(lambda x: centerwl_lut_msi[x])\n",
    "corr_mat_msi = corr_mat_msi.sort_values('wavelength')\n",
    "\n",
    "corr_mat = data_oli.corr().stack().reset_index(name=\"correlation\")\n",
    "corr_mat = corr_mat.loc[corr_mat.level_0=='value'].reset_index().drop(columns=['level_0', 'index']).rename(columns={'level_1': 'band'})\n",
    "corr_mat_oli = corr_mat.loc[corr_mat.band!='value']\n",
    "corr_mat_oli['band'] = corr_mat_oli.band.apply(lambda x: x.split('_')[0])\n",
    "corr_mat_oli['wavelength'] = corr_mat_oli.band.apply(lambda x: centerwl_lut_oli[x])\n",
    "corr_mat_oli = corr_mat_oli.sort_values('wavelength')\n",
    "\n",
    "hline = hv.HLine(0).opts(\n",
    "    color='grey', \n",
    "    line_dash='dashed', \n",
    "    line_width=1.0,\n",
    ")\n",
    "\n",
    "clim = (-1, 1)\n",
    "\n",
    "plot_msi =  hline *\\\n",
    "            corr_mat_msi.hvplot.line(x='wavelength', y='correlation', clim=clim) *\\\n",
    "            corr_mat_msi.hvplot.scatter(x='wavelength', y='correlation', \n",
    "                xlabel='Wavelength (nm)', ylabel=\"Correlation\\n(Pearson's R)\",\n",
    "                color='correlation', size=100, clim=clim, cmap='bwr',\n",
    "                title='Sentinel-2 - Rrs-TSM correlation',\n",
    "                line_color='black',\n",
    "                xlim=(400,1500), rot=45) * \\\n",
    "            corr_mat_msi.hvplot.labels(x='wavelength', y='correlation', text='band', text_baseline='top')\n",
    "\n",
    "plot_oli =  hline *\\\n",
    "            corr_mat_oli.hvplot.line(x='wavelength', y='correlation', clim=clim) *\\\n",
    "            corr_mat_oli.hvplot.scatter(x='wavelength', y='correlation', \n",
    "                xlabel='Wavelength (nm)', ylabel=\"Correlation\\n(Pearson's R)\",\n",
    "                color='correlation', size=100, clim=clim, cmap='bwr',\n",
    "                title='Landsat-8 OLI - Rrs-TSM correlation',\n",
    "                line_color='black',\n",
    "                xlim=(400,1500), rot=45) * \\\n",
    "            corr_mat_oli.hvplot.labels(x='wavelength', y='correlation', text='band', text_baseline='top')\n",
    "\n",
    "hv.Layout([plot_msi, plot_oli]).cols(1).opts(hv.opts.Scatter(clim=(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e37c8e",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c89af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "platform = ['LANDSAT-8', 'LANDSAT-9']\n",
    "platform = ['SENTINEL-2']\n",
    "\n",
    "features = df_rs.copy()\n",
    "\n",
    "# filter data\n",
    "features = features.loc[features.CLOUD_COVER<=80]\n",
    "features = features.loc[features.roi_coverage>=75]\n",
    "features = features.loc[~np.isnan(features.value)]\n",
    "features = features.loc[features.platform.isin(platform)]\n",
    "#features = features.loc[features.data_source=='mrc']\n",
    "\n",
    "# get quantile bins for stratified sampling\n",
    "features['q_bin'] = pd.qcut(features['value'], q=10)\n",
    "qbins = np.array(features['q_bin'])\n",
    "# features = features[features.columns[(features.columns.str.startswith('B'))| \\\n",
    "#                                      (features.columns=='value')| \\\n",
    "#                                      (features.columns.str.startswith('bratio'))]]\n",
    "#features = features[features.columns[(features.columns.str.startswith('B'))|(features.columns=='value')]]\n",
    "#features = features[features.columns[(features.columns.str.endswith('_median'))|(features.columns=='value')]]\n",
    "features = features.drop(columns=['system:index', 'data_source', 'platform', 'station_id', \n",
    "                                  'dt_utc', 'dt_loc', 'dt_diff', 'date', 'dt_loc_insitu', 'q_bin'], errors='ignore')\n",
    "features = features.dropna(axis=1)\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(features['value'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = features.drop('value', axis = 1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n",
    "                                                                            test_size=0.20, \n",
    "                                                                            stratify=qbins, \n",
    "                                                                            random_state=42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators=1000, random_state=42, n_jobs=-1, verbose=True)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'mg/L.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(test_labels, predictions))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(test_labels, predictions))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(test_labels, predictions)))\n",
    "mape = np.mean(np.abs((test_labels - predictions) / np.abs(test_labels)))\n",
    "print('Mean Absolute Percentage Error (MAPE):', round(mape * 100, 2))\n",
    "print('Accuracy:', round(100*(1 - mape), 2))\n",
    "\n",
    "# plot the results\n",
    "plt.plot(test_labels, predictions, '.', label=\"data\")\n",
    "plt.plot(sorted(test_labels), sorted(test_labels), '--', label=\"1:1\")\n",
    "plt.ylabel('$TSS_{insitu}$ (mg/L)')\n",
    "plt.xlabel('$TSS_{RF}$ (mg/L)')\n",
    "plt.title(\"RF validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233c2cf",
   "metadata": {},
   "source": [
    "## Feature importance based on mean decrease in impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c616d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(importances, index=feature_list).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease\\nin impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5f216",
   "metadata": {},
   "source": [
    "## Feature importance based on feature permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b038c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    rf, features, labels, n_repeats=10, random_state=42, n_jobs=-1,\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_list).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad27342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838c2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
